import os
import time
import requests
from bs4 import BeautifulSoup
from waybackpy import WaybackMachineCDXServerAPI
from tqdm import tqdm
from datetime import datetime
from selenium import webdriver  # æ·»åŠ æ­¤è¡Œ
from selenium.webdriver.edge.options import Options  # æ·»åŠ æ­¤è¡Œ
from selenium.webdriver.edge.service import Service
from webdriver_manager.microsoft import EdgeChromiumDriverManager
import page


import concurrent.futures

def extract_wayback_prefix(url):
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªhttpå‡ºç°çš„ä½ç½®
    first_http_index = url.find('http')
    if first_http_index == -1:
        return None  # æœªæ‰¾åˆ°http
    
    # ä»ç¬¬ä¸€ä¸ªhttpä¹‹åå¼€å§‹æŸ¥æ‰¾ç¬¬äºŒä¸ªhttp
    second_http_index = url.find('http', first_http_index + 1)
    if second_http_index == -1:
        return None  # æœªæ‰¾åˆ°ç¬¬äºŒä¸ªhttp
    
    # æå–ç›´åˆ°ç¬¬äºŒä¸ªhttpä¹‹å‰çš„éƒ¨åˆ†
    return url[:second_http_index]

def try_click_user_agreement(driver, wait_time=10):
    """ä¸“é—¨é’ˆå¯¹å°çº¢ä¹¦ç”¨æˆ·åè®®é“¾æ¥çš„ç‚¹å‡»å‡½æ•°"""
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    
    try:
        # 1. ä½¿ç”¨é“¾æ¥æ–‡æœ¬ç²¾ç¡®åŒ¹é…
        element = WebDriverWait(driver, wait_time).until(
            EC.element_to_be_clickable((By.LINK_TEXT, "ã€Šç”¨æˆ·åè®®ã€‹"))
        )
        element.click()
        print("æˆåŠŸé€šè¿‡é“¾æ¥æ–‡æœ¬ç‚¹å‡»ã€Šç”¨æˆ·åè®®ã€‹")
        return True
    except Exception as e:
        print(f"æ–¹æ³•1å¤±è´¥: {e}")
        
        try:
            # 2. ä½¿ç”¨éƒ¨åˆ†é“¾æ¥æ–‡æœ¬
            element = WebDriverWait(driver, wait_time).until(
                EC.element_to_be_clickable((By.PARTIAL_LINK_TEXT, "ç”¨æˆ·åè®®"))
            )
            element.click()
            print("æˆåŠŸé€šè¿‡éƒ¨åˆ†é“¾æ¥æ–‡æœ¬ç‚¹å‡»ã€Šç”¨æˆ·åè®®ã€‹")
            return True
        except Exception as e:
            print(f"æ–¹æ³•2å¤±è´¥: {e}")
            
            try:
                # 3. ä½¿ç”¨XPathå®šä½åŒ…å«ç‰¹å®šhrefçš„é“¾æ¥
                element = WebDriverWait(driver, wait_time).until(
                    EC.element_to_be_clickable((By.XPATH, 
                        "//a[contains(@href, 'agree.xiaohongshu.com/h5/terms') or contains(text(), 'ç”¨æˆ·åè®®')]"))
                )
                element.click()
                print("æˆåŠŸé€šè¿‡XPathç‚¹å‡»ã€Šç”¨æˆ·åè®®ã€‹")
                return True
            except Exception as e:
                print(f"æ–¹æ³•3å¤±è´¥: {e}")
                
                try:
                    # 4. ä½¿ç”¨CSSé€‰æ‹©å™¨
                    element = WebDriverWait(driver, wait_time).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, "a.links"))
                    )
                    if "ç”¨æˆ·åè®®" in element.text:
                        element.click()
                        print("æˆåŠŸé€šè¿‡CSSé€‰æ‹©å™¨ç‚¹å‡»ã€Šç”¨æˆ·åè®®ã€‹")
                        return True
                except Exception as e:
                    print(f"æ–¹æ³•4å¤±è´¥: {e}")
                    
                    try:
                        # 5. ä½¿ç”¨JavaScriptæ‰§è¡Œç‚¹å‡»
                        links = driver.find_elements(By.XPATH, "//a[contains(text(), 'ç”¨æˆ·åè®®')]")
                        if links:
                            driver.execute_script("arguments[0].click();", links[0])
                            print("æˆåŠŸé€šè¿‡JavaScriptç‚¹å‡»ã€Šç”¨æˆ·åè®®ã€‹")
                            return True
                        else:
                            print("æœªæ‰¾åˆ°ã€Šç”¨æˆ·åè®®ã€‹é“¾æ¥")
                    except Exception as e:
                        print(f"æ–¹æ³•5å¤±è´¥: {e}")
    
    # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
    print("æ— æ³•ç‚¹å‡»ã€Šç”¨æˆ·åè®®ã€‹é“¾æ¥")
    return False
def process_snapshot(snapshot, index):
    try:
        archived_url = snapshot.archive_url
        print(f"å¤„ç†ç¬¬ {index+1} æ¡: {archived_url}")
        prefix = extract_wayback_prefix(archived_url)
        print(f"æå–çš„å‰ç¼€: {prefix}")
        
        driver = page.open_url_with_edge(archived_url, headless=True, wait_time=0)

        flag0 = False
         # å°è¯•ç‚¹å‡»"ç”¨æˆ·åè®®"å…ƒç´  - æ·»åŠ ä»¥ä¸‹ä»£ç 
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(2)
            
            # æ–¹æ³•1: é€šè¿‡CSSé€‰æ‹©å™¨
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            
            # å°è¯•ä½¿ç”¨CSSé€‰æ‹©å™¨
            try:
                element = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "li.navigation-item.sign-up"))
                )
                element.click()
                print("æˆåŠŸç‚¹å‡»ç”¨æˆ·åè®®æŒ‰é’®")
                flag0 = True
            except:
                # å°è¯•ä½¿ç”¨XPath
                try:
                    element = WebDriverWait(driver, 10).until(
                        EC.element_to_be_clickable((By.XPATH, "//li[contains(text(), 'ç”¨æˆ·åè®®')]"))
                    )
                    element.click()
                    print("æˆåŠŸé€šè¿‡XPathç‚¹å‡»ç”¨æˆ·åè®®æŒ‰é’®")
                    flag0 = True
                except:
                    # å°è¯•JavaScriptç‚¹å‡»
                    try:
                        elements = driver.find_elements(By.XPATH, "//*[contains(text(), 'ç”¨æˆ·åè®®')]")
                        if elements:
                            driver.execute_script("arguments[0].click();", elements[0])
                            print("æˆåŠŸé€šè¿‡JavaScriptç‚¹å‡»ç”¨æˆ·åè®®æŒ‰é’®")
                            flag0 = True
                        else:
                            print("æœªæ‰¾åˆ°ç”¨æˆ·åè®®æŒ‰é’®")
                    except Exception as click_error:
                        print(f"ç‚¹å‡»ç”¨æˆ·åè®®å¤±è´¥: {click_error}")
                        
            # ç­‰å¾…æ–°å†…å®¹åŠ è½½
            time.sleep(2)

        except Exception as e:
            print(f"å°è¯•ç‚¹å‡»ç”¨æˆ·åè®®æ—¶å‘ç”Ÿé”™è¯¯: {e}")


        if not flag0:
            # å¦‚æœç‚¹å‡»å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ try_click_user_agreement å‡½æ•°
            if not try_click_user_agreement(driver):
                print("æ— æ³•ç‚¹å‡»ã€Šç”¨æˆ·åè®®ã€‹é“¾æ¥ï¼Œè·³è¿‡æ­¤å¿«ç…§")
                driver.quit()
                return False, f"ç¬¬ {index+1} æ¡å¤±è´¥ï¼šæ— æ³•ç‚¹å‡»ã€Šç”¨æˆ·åè®®ã€‹é“¾æ¥"

        html_content = driver.page_source
        driver.quit()
        # print(res)
        soup = BeautifulSoup(html_content, 'html.parser') 
        text = soup.get_text(separator='\n')

        # è®¾ç½®ä¿å­˜æ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        timestamp = snapshot.timestamp
        date_str = datetime.strptime(timestamp, "%Y%m%d%H%M%S").strftime("%Y-%m-%d")
        filename = f"xiaohongshu_agreement_{date_str}.txt"
        file_path = os.path.join(save_folder, filename)

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(text)

        print(f"âœ… ç¬¬ {index+1} æ¡ä¿å­˜æˆåŠŸï¼š{filename}")
        
        # å¤„ç†ä¿å­˜é€»è¾‘...
        return True, index
    except Exception as e:
        return False, f"ç¬¬ {index+1} æ¡å¤±è´¥ï¼š{e}"



# 1ï¸âƒ£ è®¾ç½®ç›®æ ‡åè®®é¡µé¢ URL
url = "https://www.xiaohongshu.com/explore"  # â† ä½ å¯ä»¥æ¢æˆåˆ«çš„å¹³å°é“¾æ¥
user_agent = "Mozilla/5.0"


# 2ï¸âƒ£ åœ¨æ¡Œé¢æ–°å»ºä¿å­˜æ–‡ä»¶å¤¹
desktop_path = "./test/"
save_folder = os.path.join(desktop_path, "xiaohongshu_User_Agreements")
os.makedirs(save_folder, exist_ok=True)

# 3ï¸âƒ£ åˆå§‹åŒ– Wayback æŠ“å–å™¨ï¼Œè·å–å…¨éƒ¨å¿«ç…§
wayback = WaybackMachineCDXServerAPI(url, user_agent)
snapshots = list(wayback.snapshots())
print(f"ğŸ“¦ å…±å‘ç° {len(snapshots)} æ¡å†å²å¿«ç…§ï¼Œå¼€å§‹æŠ“å–...\n")

# æŒ‰å‘¨åˆ†ç»„ï¼Œæ¯å‘¨åªä¿ç•™ç¬¬ä¸€æ¡è®°å½•


weekable = True
filtered_snapshots = []
if weekable:
    weekly_snapshots = {}
    for snapshot in snapshots:
        timestamp = snapshot.timestamp
        # è½¬æ¢ä¸ºæ—¥æœŸå¯¹è±¡ä»¥è·å–å¹´å’Œå‘¨æ•°
        date_obj = datetime.strptime(timestamp, "%Y%m%d%H%M%S")
        # isocalendar() è¿”å› (å¹´, å‘¨, æ˜ŸæœŸå‡ )ï¼Œæˆ‘ä»¬åªå–å‰ä¸¤ä¸ª
        year, week_num = date_obj.isocalendar()[:2]
        # ä½¿ç”¨(å¹´,å‘¨)ä½œä¸ºé”®
        week_key = f"{year}_{week_num:02d}"
        
        # å¦‚æœè¿™ä¸€å‘¨è¿˜æ²¡æœ‰è®°å½•ï¼Œåˆ™æ·»åŠ è¯¥å¿«ç…§
        if week_key not in weekly_snapshots:
            weekly_snapshots[week_key] = snapshot
            # æŒ‰æ—¶é—´é¡ºåºæ’åº
    filtered_snapshots = [weekly_snapshots[k] for k in sorted(weekly_snapshots.keys())]
    print(f"ğŸ“Š ç­›é€‰åæ¯å‘¨ä¸€æ¡ï¼Œå…± {len(filtered_snapshots)} æ¡å¿«ç…§\n")
else:
    monthly_snapshots = {}
    for snapshot in snapshots:
        timestamp = snapshot.timestamp
        # æå–å¹´æœˆä½œä¸ºé”®ï¼Œæ ¼å¼ä¸º "YYYYMM"
        month_key = timestamp[:6]
        
        # å¦‚æœè¿™ä¸ªæœˆä»½è¿˜æ²¡æœ‰è®°å½•ï¼Œåˆ™æ·»åŠ è¯¥å¿«ç…§
        if month_key not in monthly_snapshots:
            monthly_snapshots[month_key] = snapshot
            # æŒ‰æ—¶é—´é¡ºåºæ’åº
    filtered_snapshots = [monthly_snapshots[k] for k in sorted(monthly_snapshots.keys())]
    print(f"ğŸ“Š ç­›é€‰åæ¯æœˆä¸€æ¡ï¼Œå…± {len(filtered_snapshots)} æ¡å¿«ç…§\n")




begin = 100
end = 1000


# ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
    futures = {executor.submit(process_snapshot, snapshot, i): i 
               for i, snapshot in enumerate(filtered_snapshots) 
               if begin <= i <= end}
    
    for future in concurrent.futures.as_completed(futures):
        success, result = future.result()
        if success:
            print(f"âœ… ç¬¬ {result+1} æ¡å¤„ç†æˆåŠŸ")
        else:
            print(f"âš ï¸ {result}")

# # 4ï¸âƒ£ å¾ªç¯æŠ“å– + é˜²å° + é”™è¯¯è·³è¿‡ + å®æ—¶æç¤º
# for i, snapshot in enumerate(tqdm(filtered_snapshots, desc="æŠ“å–åè®®ä¸­ï¼ˆæ¯å‘¨ä¸€æ¡ï¼‰")):
#     if i < begin or i > end:
#         continue  # è·³è¿‡ä¸åœ¨èŒƒå›´å†…çš„å¿«ç…§
#     try:
#         archived_url = snapshot.archive_url
#         print(archived_url)

#             # æ‰“å¼€æµè§ˆå™¨å¹¶åŠ è½½é¡µé¢
#         driver = page.open_url_with_chrome(archived_url, headless=False, wait_time=0)
        
#         page.try_method_click(driver,"æœåŠ¡åè®®")

#         # è·å–æ¸²æŸ“åçš„HTML
#         html_content = driver.page_source
#         # ä½¿ç”¨å®Œåå…³é—­
#         driver.quit()
#         # print(res)
#         soup = BeautifulSoup(html_content, 'html.parser') 
#         text = soup.get_text(separator='\n')

#         # è®¾ç½®ä¿å­˜æ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
#         timestamp = snapshot.timestamp
#         date_str = datetime.strptime(timestamp, "%Y%m%d%H%M%S").strftime("%Y-%m-%d")
#         filename = f"xiaohongshu_agreement_{date_str}.txt"
#         file_path = os.path.join(save_folder, filename)

#         with open(file_path, 'w', encoding='utf-8') as f:
#             f.write(text)

#         print(f"âœ… ç¬¬ {i+1} æ¡ä¿å­˜æˆåŠŸï¼š{filename}")
#         time.sleep(1.5)  # é™é€Ÿé˜²å°

#     except Exception as e:
#         print(f"âš ï¸ ç¬¬ {i+1} æ¡å¤±è´¥ï¼š{e}")
#         time.sleep(2)

print(f"\nğŸ‰ å…¨éƒ¨æŠ“å–å®Œæ¯•ï¼å…±å¤„ç† {len(snapshots)} æ¡ã€‚\nğŸ“ æ–‡ä»¶ä¿å­˜åœ¨æ¡Œé¢æ–‡ä»¶å¤¹ï¼š{save_folder}")