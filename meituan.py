import os
import time
import requests
from bs4 import BeautifulSoup
from waybackpy import WaybackMachineCDXServerAPI
from tqdm import tqdm
from datetime import datetime
from selenium import webdriver  # æ·»åŠ æ­¤è¡Œ
from selenium.webdriver.edge.options import Options  # æ·»åŠ æ­¤è¡Œ
from selenium.webdriver.edge.service import Service
from webdriver_manager.microsoft import EdgeChromiumDriverManager
import page


import concurrent.futures

def extract_wayback_prefix(url):
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªhttpå‡ºç°çš„ä½ç½®
    first_http_index = url.find('http')
    if first_http_index == -1:
        return None  # æœªæ‰¾åˆ°http
    
    # ä»ç¬¬ä¸€ä¸ªhttpä¹‹åå¼€å§‹æŸ¥æ‰¾ç¬¬äºŒä¸ªhttp
    second_http_index = url.find('http', first_http_index + 1)
    if second_http_index == -1:
        return None  # æœªæ‰¾åˆ°ç¬¬äºŒä¸ªhttp
    
    # æå–ç›´åˆ°ç¬¬äºŒä¸ªhttpä¹‹å‰çš„éƒ¨åˆ†
    return url[:second_http_index]

def process_snapshot(snapshot, index):
    try:
        archived_url = snapshot.archive_url
        print(f"å¤„ç†ç¬¬ {index+1} æ¡: {archived_url}")
        prefix = extract_wayback_prefix(archived_url)
        print(f"æå–çš„å‰ç¼€: {prefix}")
        
        driver = page.open_url_with_edge(archived_url, headless=True, wait_time=0)

        html_content = driver.page_source
        driver.quit()
        # print(res)
        soup = BeautifulSoup(html_content, 'html.parser') 
        text = soup.get_text(separator='\n')

        # è®¾ç½®ä¿å­˜æ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        timestamp = snapshot.timestamp
        date_str = datetime.strptime(timestamp, "%Y%m%d%H%M%S").strftime("%Y-%m-%d")
        filename = f"meituan_agreement_{date_str}.txt"
        file_path = os.path.join(save_folder, filename)

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(text)

        print(f"âœ… ç¬¬ {index+1} æ¡ä¿å­˜æˆåŠŸï¼š{filename}")
        
        # å¤„ç†ä¿å­˜é€»è¾‘...
        return True, index
    except Exception as e:
        return False, f"ç¬¬ {index+1} æ¡å¤±è´¥ï¼š{e}"



# 1ï¸âƒ£ è®¾ç½®ç›®æ ‡åè®®é¡µé¢ URL
url = "https://rules-center.meituan.com/rules-detail/4"  # â† ä½ å¯ä»¥æ¢æˆåˆ«çš„å¹³å°é“¾æ¥
user_agent = "Mozilla/5.0"


# 2ï¸âƒ£ åœ¨æ¡Œé¢æ–°å»ºä¿å­˜æ–‡ä»¶å¤¹
desktop_path = "./test/"
save_folder = os.path.join(desktop_path, "meituan_User_Agreements")
os.makedirs(save_folder, exist_ok=True)

# 3ï¸âƒ£ åˆå§‹åŒ– Wayback æŠ“å–å™¨ï¼Œè·å–å…¨éƒ¨å¿«ç…§
wayback = WaybackMachineCDXServerAPI(url, user_agent)
snapshots = list(wayback.snapshots())
print(f"ğŸ“¦ å…±å‘ç° {len(snapshots)} æ¡å†å²å¿«ç…§ï¼Œå¼€å§‹æŠ“å–...\n")

# æŒ‰å‘¨åˆ†ç»„ï¼Œæ¯å‘¨åªä¿ç•™ç¬¬ä¸€æ¡è®°å½•


weekable = False
filtered_snapshots = []
if weekable:
    weekly_snapshots = {}
    for snapshot in snapshots:
        timestamp = snapshot.timestamp
        # è½¬æ¢ä¸ºæ—¥æœŸå¯¹è±¡ä»¥è·å–å¹´å’Œå‘¨æ•°
        date_obj = datetime.strptime(timestamp, "%Y%m%d%H%M%S")
        # isocalendar() è¿”å› (å¹´, å‘¨, æ˜ŸæœŸå‡ )ï¼Œæˆ‘ä»¬åªå–å‰ä¸¤ä¸ª
        year, week_num = date_obj.isocalendar()[:2]
        # ä½¿ç”¨(å¹´,å‘¨)ä½œä¸ºé”®
        week_key = f"{year}_{week_num:02d}"
        
        # å¦‚æœè¿™ä¸€å‘¨è¿˜æ²¡æœ‰è®°å½•ï¼Œåˆ™æ·»åŠ è¯¥å¿«ç…§
        if week_key not in weekly_snapshots:
            weekly_snapshots[week_key] = snapshot
            # æŒ‰æ—¶é—´é¡ºåºæ’åº
    filtered_snapshots = [weekly_snapshots[k] for k in sorted(weekly_snapshots.keys())]
    print(f"ğŸ“Š ç­›é€‰åæ¯å‘¨ä¸€æ¡ï¼Œå…± {len(filtered_snapshots)} æ¡å¿«ç…§\n")
else:
    monthly_snapshots = {}
    for snapshot in snapshots:
        timestamp = snapshot.timestamp
        # æå–å¹´æœˆä½œä¸ºé”®ï¼Œæ ¼å¼ä¸º "YYYYMM"
        month_key = timestamp[:6]
        
        # å¦‚æœè¿™ä¸ªæœˆä»½è¿˜æ²¡æœ‰è®°å½•ï¼Œåˆ™æ·»åŠ è¯¥å¿«ç…§
        if month_key not in monthly_snapshots:
            monthly_snapshots[month_key] = snapshot
            # æŒ‰æ—¶é—´é¡ºåºæ’åº
    filtered_snapshots = [monthly_snapshots[k] for k in sorted(monthly_snapshots.keys())]
    print(f"ğŸ“Š ç­›é€‰åæ¯æœˆä¸€æ¡ï¼Œå…± {len(filtered_snapshots)} æ¡å¿«ç…§\n")




begin = 0
end = 1000


# ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
    futures = {executor.submit(process_snapshot, snapshot, i): i 
               for i, snapshot in enumerate(filtered_snapshots) 
               if begin <= i <= end}
    
    for future in concurrent.futures.as_completed(futures):
        success, result = future.result()
        if success:
            print(f"âœ… ç¬¬ {result+1} æ¡å¤„ç†æˆåŠŸ")
        else:
            print(f"âš ï¸ {result}")

# # 4ï¸âƒ£ å¾ªç¯æŠ“å– + é˜²å° + é”™è¯¯è·³è¿‡ + å®æ—¶æç¤º
# for i, snapshot in enumerate(tqdm(filtered_snapshots, desc="æŠ“å–åè®®ä¸­ï¼ˆæ¯å‘¨ä¸€æ¡ï¼‰")):
#     if i < begin or i > end:
#         continue  # è·³è¿‡ä¸åœ¨èŒƒå›´å†…çš„å¿«ç…§
#     try:
#         archived_url = snapshot.archive_url
#         print(archived_url)

#             # æ‰“å¼€æµè§ˆå™¨å¹¶åŠ è½½é¡µé¢
#         driver = page.open_url_with_chrome(archived_url, headless=False, wait_time=0)
        
#         page.try_method_click(driver,"æœåŠ¡åè®®")

#         # è·å–æ¸²æŸ“åçš„HTML
#         html_content = driver.page_source
#         # ä½¿ç”¨å®Œåå…³é—­
#         driver.quit()
#         # print(res)
#         soup = BeautifulSoup(html_content, 'html.parser') 
#         text = soup.get_text(separator='\n')

#         # è®¾ç½®ä¿å­˜æ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
#         timestamp = snapshot.timestamp
#         date_str = datetime.strptime(timestamp, "%Y%m%d%H%M%S").strftime("%Y-%m-%d")
#         filename = f"meituan_agreement_{date_str}.txt"
#         file_path = os.path.join(save_folder, filename)

#         with open(file_path, 'w', encoding='utf-8') as f:
#             f.write(text)

#         print(f"âœ… ç¬¬ {i+1} æ¡ä¿å­˜æˆåŠŸï¼š{filename}")
#         time.sleep(1.5)  # é™é€Ÿé˜²å°

#     except Exception as e:
#         print(f"âš ï¸ ç¬¬ {i+1} æ¡å¤±è´¥ï¼š{e}")
#         time.sleep(2)

print(f"\nğŸ‰ å…¨éƒ¨æŠ“å–å®Œæ¯•ï¼å…±å¤„ç† {len(snapshots)} æ¡ã€‚\nğŸ“ æ–‡ä»¶ä¿å­˜åœ¨æ¡Œé¢æ–‡ä»¶å¤¹ï¼š{save_folder}")